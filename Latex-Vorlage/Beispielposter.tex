\documentclass{tikzposter}


% \usetheme{Default}
% \usetheme{Rays}
\usetheme{Basic}
% \usecolorstyle[colorOne=white,colorTwo=OCGreen,colorThree=blue]{Default}
% \usetheme{Simple}
% \usetheme{Envelope}
% \usetheme{Wave}
% \usetheme{Board}
% \usetheme{Autumn}
% \usetheme{Desert}
\definecolorstyle{OC}{%
  \colorlet{colorOne}{black!2}
  \definecolor{colorTwo}{HTML}{4ca83d} % chair logo colour (from svg logo)
  \colorlet{colorThree}{black}
}{%
  % Background Colors
  \colorlet{backgroundcolor}{colorOne}
  \colorlet{framecolor}{colorTwo}
  % Title Colors
  \colorlet{titlefgcolor}{colorOne}
  \colorlet{titlebgcolor}{colorTwo}
  % Block Colors
  \colorlet{blocktitlebgcolor}{colorTwo}
  \colorlet{blocktitlefgcolor}{colorOne}
  \colorlet{blockbodybgcolor}{colorOne}
  \colorlet{blockbodyfgcolor}{colorThree}
  % Innerblock Colors
  \colorlet{innerblocktitlebgcolor}{colorOne}
  \colorlet{innerblocktitlefgcolor}{colorThree}
  \colorlet{innerblockbodybgcolor}{colorTwo}
  \colorlet{innerblockbodyfgcolor}{colorOne}
  % Note colors
  \colorlet{notefgcolor}{colorOne}
  \colorlet{notebgcolor}{colorTwo}
  \colorlet{noteframecolor}{colorThree}
}
\usecolorstyle{OC}
\definetitlestyle{Basic}{
    width=770mm, roundedcorners=0, linewidth=0pt, innersep=10pt,
    titletotopverticalspace=20mm, titletoblockverticalspace=20mm,
    titlegraphictotitledistance=25mm, titletextscale=1
}{
    \coordinate (topright) at (0.5\textwidth-0.5\titlelinewidth, 0.5\textheight-0.5\titlelinewidth);
    % Original value:
    % \coordinate (bottomleft) at (-0.5\textwidth+0.5\titlelinewidth, 0.5\textheight-\titlegraphicheight-2\titletotopverticalspace-2\titleinnersep);
    \coordinate (bottomleft) at (-0.5\textwidth+0.5\titlelinewidth, 0.5\textheight-13cm);
    \draw[line width=\titlelinewidth, inner sep=\titleinnersep, fill=titlebgcolor] (bottomleft) rectangle (topright);
    % \node [fill=black!3,anchor=north east] at (topright) {\includegraphics{Logo.pdf}};
    % \node [fill=black!3,anchor=south west] at (-0.5\textwidth, -0.5\textheight) {\includegraphics{Logo.pdf}};
    % \node [fill=black!3,anchor=south east] at (0.5\textwidth, -0.5\textheight) {\includegraphics{Logo.pdf}};
    \node [fill=black!3,anchor=north east,rounded corners=1cm,inner sep=1cm] at (0.5\textwidth-1.5cm, 0.5\textheight-1.5cm) {\includegraphics[width=7cm]{Logo.pdf}};
}


\usepackage{blindtext}


\usepackage{comment}


\usepackage{amssymb}


\usepackage{amsmath}


\usepackage{enumitem}
\setitemize{labelindent=1ex,labelsep=1ex,leftmargin=*}


\newcommand{\alert}[1]{{\bfseries\color{colorTwo}#1}}


\newcommand{\lcs}{SupRB}
\newcommand{\lcsone}{SupRB-1}


% suffix with “p” to denote system predictions (they get a hat)
\newcommand{\XX}{\mathcal{X}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\XXAA}{\{\mathcal{X}, \mathcal{A}\}^T}
\newcommand{\Xeval}{X_{\text{eval}}}
\newcommand{\xdim}{{D_\XX}}
\newcommand{\adim}{{D_\AA}}
\newcommand{\XA}{\{X, A\}^T}
\newcommand{\xa}{\{x, a\}^T}
\newcommand{\XAtrain}{\{X, A\}^T_\text{train}}
\newcommand{\XAvalid}{\{X, A\}^T_\text{valid}}
\newcommand{\qtrain}{q_\text{train}}
\newcommand{\qvalid}{q_\text{valid}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\amax}{a_{\text{max}}}
\newcommand{\amaxp}{\hat{a}_{\text{max}}}
\newcommand{\amaxpc}{\hat{a}_{\text{max}_c}}
\newcommand{\qp}{\hat{q}}
\newcommand{\TT}{\mathtt{T}}
\newcommand{\FF}{\mathtt{F}}

\newcommand{\xat}[1][a]{\{x, {#1}\}^T}
\newcommand{\q}[1][a]{q(x, a)}
\newcommand{\qc}[1][a]{q_c(\xat[{#1}])}
\newcommand{\qstar}[1][a]{q_*(\xat[{#1}])}
\newcommand{\qq}{\hat{q}}
\newcommand{\wc}{w_c}
\renewcommand{\aa}{\hat{a}_*}


\title{Towards a Pittsburgh-style LCS for Learning Manufacturing Machinery Parametrizations}
\author{Michael Heider, David Pätzel, Jörg Hähner}
\settitle{
  \begin{center}
    \vbox{
      \color{titlefgcolor}
      {\bfseries\Huge\sc
        Towards a Pittsburgh-style LCS for Learning\\
        Manufacturing Machinery Parametrizations
        \par
      }
      \vspace*{1em}
      {\huge \@author \par}
      \vspace*{1em}
      {\LARGE \@institute}
    }
  \end{center}
}


\institute{University of Augsburg, Germany}


\begin{document}


\maketitle


\begin{columns}


\column{0.5}
%\block{Abstract}{%
%  We present a first evaluation of a new accuracy-based Pittsburgh-style learning classifier system (LCS) for supervised learning of multi-dimensional continuous decision problems: The \lcsone{} (\textbf{Sup}\-er\-vi\-sed \textbf{R}ule-\textbf{B}ased) learning system.
%  Designed primarily for finding parametrizations for industrial machinery, \lcsone{} learns an approximation of a continuous quality function from examples (consisting of situations, choices and associated qualities—all continous, the first two possibly multi-dimensional) and is then able to make an optimal choice as well as predict the quality of a choice in a given situation.
%  This paper shows and discusses preliminary results of \lcsone{}'s performance on an additive manufacturing problem.
%}
\block{Motivation}{%
  \begin{itemize}
    \item \alert{How to parametrize industrial machinery} is often learned by human operators through experimental exploration based on prior knowledge.
    \item \alert{Transferring such acquired knowledge} to other operators can be challenging:
      Humans have a hard time to
      \begin{itemize}
        \item exactly attribute parametrization choices to situations and
        \item communicate knowledge per se.
      \end{itemize}
    \item \alert{Supervised learning} (e.\,g.\ on datapoints generated during exploration) can support this process.
    \item To be accepted by operators, the system needs to be \alert{comprehensible}.
    \item \alert{Learning Classifier Systems} comprehensibly create a global model from multiple interpretable less complex local models.
  \end{itemize}
}


\column{0.5}
\block{\lcsone{} in brief}{%
  \begin{itemize}
    \item \alert{Accuracy-based Pittsburgh-style} Learning Classifier System
    \item Supervised learning on \alert{continuous} multi\--dimension\-al decision problems
      \begin{displaymath}
        q : \XX \times \AA \to \RR, \quad \XX \subset \RR^{n}, \AA \subset \RR^{m}
      \end{displaymath}
    \item Global models
      \begin{itemize}
        \item evolved by a \alert{genetic algorithm}
        \item consist of a set of localized simplistic paraboloid (i.\,e.\ linear regression) models each
      \end{itemize}
      % David: Maybe “model structure search” could help
    \item Fitness measure: \alert{Bayesian Information Criterion}
      \begin{itemize}
        \item prediction quality (accuracy) and
        \item model complexity (number of parameters to fit)
      \end{itemize}
    \item Predictions:
      \begin{itemize}
        \item given $x \in \XX$, location of the \alert{mode of $q_{x} : \AA \to \RR $}
        \item values of $q(x, a)$ (basic \alert{function approximation})
      \end{itemize}
  \end{itemize}
}


\end{columns}


\begin{columns}


\column{0.64}
\block{Application}{%
  \begin{itemize}
      % Application archetype
    % \item \lcsone{} is evaluated on an abstract representation of an \alert{FDM-based additive manufacturing} (AM) process.
    \item \alert{FDM-based additive manufacturing processes}
      \begin{itemize}
        % \item Optimal process parameters (\alert{parametrizations}) depend on machine and material properties as well as environmental conditions.
        \item \alert{Optimal parametrizations} depend on machine and material properties as well as environmental conditions.
        % \item Parametrizations might have to be fine tuned quite frequently, depending on the required part quality, making traditional optimization (where the problem is evaluated multiple times until an optimum is found) too costly.
        \item Parametrizations might have to be \alert{fine tuned frequently} depending on the required part quality. $\Rightarrow$ Traditional optimization (problem evaluated multiple times until optimum found) is usually too costly.
        % \item Operators are often inexperienced with little knowledge about the process itself.
        % David: unsure why this is here (it's kind of in the motivation already)
      \end{itemize}
    \item Quality function of an \alert{abstract additive manufacturing process}: AM-Gauss function
      \begin{itemize}
        % \item We reduce the problem dimensions to the most important aspects for situation $(x_1,\dots , x_5)$ as well as parametrization $(a_1, \dots ,a_6)$.
        \item Consider abstract situations $(x_1, \dots, x_5)$ and parametrizations $(a_1, \dots,a_6)$ with realistic dimensionalities (derived from expert knowledge).
          % David: Must mention “extremes” here (i.e. \in [0, 1])
        % \item Assume that an optimal choice exists for each combination of dimensions such that part quality is the highest with quality decreasing towards the extremes.
        \item Assume that a single optimum with regard to \alert{part quality} exists for each combination of dimensions with quality decreasing towards the extremes. $\Rightarrow$ Modelled as \alert{two-dimensional Gaussian functions}.
        % \item Combine those to a global Gaussian Mixture function.
        % David: I learned that Gaussian Mixture might usually be only used in probabilistic settings, i.e. it's a weighted sum of normalized(!) Gaussians with weights summing to one (maybe avoid this term—I know, we used it in the paper *.*).
        \item \alert{Sum} these Gaussian functions.
        \item Allows to actually assess the choices of \lcsone{} (impossible with real data where the true optimum is unknown).
      \end{itemize}
  \end{itemize}
}


\column{0.36}
\block{AM-Gauss function}{%
  \begin{equation*}
    q(y) =
    \sum_{\substack{j \in 1, \dots, 11,\\k \in 1, \dots, 11,\\k \neq j}}
    \exp\left({-\left(\begin{pmatrix}y_j\\y_k\end{pmatrix}-s_{j,k}\right)^T P_{j,k} \left(\begin{pmatrix}y_j\\y_k\end{pmatrix}-s_{j,k}\right)}\right)
  \end{equation*}
  \vspace{1cm}
  \begin{itemize}
    \item $y = (y_{1}, \dots, y_{11})^{T} = (x_1,\dots , x_5, a_1, \dots ,a_6)^T$
    \item each $P_{j,k}$ is a positive semi-definite matrix in $\RR^{2 \times 2}$ with eigenvalues in $[0, 30]$ (ensures sensible scaling)
    \item $s_{j,k}$ is a vector in $[-1, 1]^2$  specifying the location of the summand's mode
  \end{itemize}
}


\end{columns}


\begin{columns}


\column{0.5}
\block{Experiments}{
  \begin{itemize}
      % David: I liked the earlier stressing of it being a single one. This way it sounds a bit as if we did the random generation multiple times (the “seed 1” in the parentheses might get skimmed over).
    \item \alert{Randomly generate an AM-Gauss function} (the required $P_{j,k}$'s and $s$'s generated from random seed 1).
    \item Sample a training set containing \alert{2000 examples} (1000 for training local models, 1000 for fitness evaluations).
    \item \alert{20 runs} (consecutive random seeds), 400 generations.
    \item Evaluate after each generation
      \begin{itemize}
        \item overall \alert{goodness-of-fit} (separate holdout set of size 1000)
        \item \alert{quality of predicted optimal parametrization} compared to quality of actual optimal parametrization (in 1000 random situations)
      \end{itemize}
      % David: possibly reuse notation from above for clarification here
    \item Reported results are averages of the runs' populations' \alert{elitists}.
    \item Baseline: \alert{Two-layer fully connected artificial neural network} evaluated on identical data using 20 runs (same consecutive random seeds).
  \end{itemize}
  \begin{minipage}[t]{0.49\linewidth}
    \begin{tikzfigure}[Goodness-of-fit of \alert{quality predictions} on training and holdout data with ANN baseline.\par]
      \includegraphics[trim=0cm 0cm 1.5cm 1.4cm,clip=true,width=\linewidth]{../plots/printing-BIC-fit-rmse-trans.pdf}\label{fig:RMSEFitEval}
    \end{tikzfigure}
  \end{minipage}
  \,
  \begin{minipage}[t]{0.49\linewidth}
    \begin{tikzfigure}[Goodness-of-fit of \alert{parametrization choices} on holdout data with ANN baseline.\par]
      \includegraphics[trim=0cm 0cm 1.5cm 1.4cm,clip=true,width=\linewidth]{../plots/printing-BIC-choice-rmse-eval-trans.pdf}\label{fig:RMSEOptimum}
    \end{tikzfigure}
  \end{minipage}

  \begin{minipage}[t]{0.49\linewidth}
    \begin{tikzfigure}[Elitist's number of classifiers.\par]
      \includegraphics[trim=0cm 0cm 1.5cm 1.4cm,clip=true,width=\linewidth]{../plots/printing-BIC-pop-size-trans.pdf}
    \end{tikzfigure}
  \end{minipage}
  \,
  \begin{minipage}[t]{0.49\linewidth}
    \begin{tikzfigure}[Samples matched by default classifier only.\par]
      \includegraphics[trim=0cm 0cm 1.5cm 1.4cm,clip=true,width=\linewidth]{../plots/printing-BIC-unmatched-trans.pdf}
    \end{tikzfigure}
  \end{minipage}
}

\column{0.5}
\block{Results}{%
  \begin{enumerate}
      % David: Why enumerate here? Is the order that important? Also, we never refer to the numbers
      % Michi: Order is not important, it just felt like a stronger message
    \item \lcsone{}'s \alert{quality predictions}' RMSE on holdout data (Figure~\ref{fig:RMSEFitEval})
      \begin{itemize}
        \item improves rapidly over the first 70 generations and then seems to converge at around 1.67 which falls short of the ANN baseline.
        \item
          In contrast to the average, the best run not only converges slightly faster, but also achieves much better results at an error of about 1.1 surpassing the baseline.
          The most likely source of this is \alert{premature convergence} to local optima.
      \end{itemize}
      % David: Must introduce the term “parametrization choices” somewhere. Michi: in application
    \item Regarding the RMSE of the \alert{parametrization choices} on holdout data (Figure~\ref{fig:RMSEOptimum}),
      \begin{itemize}
        \item the average results only improve slightly over time and \alert{miss the baseline by far}.
        \item However, the best run comes close to the baseline and is able to beat several ANNs performance-wise in the process.
      \end{itemize}
    \item For \lcsone{}, \alert{good results in prediction quality} often seem to correlate with \alert{good results on choosing parametrizations}, whereas many ANNs that performed well on the former had vastly worse results on the latter.% (a behaviour also indicated by the much larger standard deviation of the baseline in Figure~\ref{fig:RMSEOptimum}).
    \item Elitists of each run converge to 10 local models with a low standard deviation, the overall best performing individuals having 11 to 12 local models.
    \item As most runs evolved solutions of similar size but with substantial differences in errors we \alert{suspect that the models were ill placed at local optima}, supporting the suspected premature convergence issue.
    \item We also shortly investigated the performance on 29 other AM-Gauss functions (seeds 2 through 30), albeit with only one run each, achieving comparable results.
  \end{enumerate}
}


\block{Improvements}{%
  We see several opportunities to increase the overall performance:
  \begin{itemize}
    \item Use \alert{non-parabolic local models} that can better fit the problem function
    \item Prefer \alert{good fit around higher predictions} of to better predict the location of optima
    \item Nurture a \alert{more healthy and diverse population} of solutions
  \end{itemize}
}


\end{columns}


\end{document}
